---
id: 1
title: Create-Physical-AI-Constitution
stage: constitution
date: 2025-12-24
surface: agent
model: claude-sonnet-4-5-20250929
feature: Physical-AI-Humanoid-Robotics-Textbook
branch: master
user: user
command: sp.constitution
labels: [constitution, textbook, physical-ai, humanoid-robotics]
links:
  spec: null
  ticket: null
  adr: null
  pr: null
files:
 - .specify/memory/constitution.md
tests:
 - No tests needed for constitution creation
---

## Prompt

```text
generate a prompt to create a constitution of this book using rules and regulations of prompt-engineering and context-engineering to create this text book:
Create a Textbook for Teaching Physical AI & Humanoid Robotics Course
The future of work will be a partnership between people, intelligent agents (AI software), and robots. This shift won't necessarily eliminate jobs but will change what humans do, leading to a massive demand for new skills. The course details are documented below:

Requirements:

You are required to complete a unified book project using Claude Code and Spec-Kit Plus. The core deliverables are:

1.    AI/Spec-Driven Book Creation: Write a book using Docusaurus and deploy it to GitHub Pages. You will use Spec-Kit Plus ( https://github.com/panaversity/spec-kit-plus/ ) and Claude Code ( https://www.claude.com/product/claude-code ) to write the book.
2.    Use two mcp servers: context7(to read docusaurus documentation) and github mcp server(to deploy on github pages).

creating and using reusable intelligence via Claude Code Subagents and Agent Skills in the book project.

.  also implement Signup and Signin using https://www.better-auth.com/ At signup you will ask questions from the user about their software and hardware background. Knowing the background of the user we will be able to personalize the content.

the logged user can personalise the content in the chapters by pressing a button at the start of each chapter.

the logged user can translate the content in Urdu in the chapters by pressing a button at the start of each chapter.

2. Integrated RAG Chatbot Development: Build and embed a Retrieval-Augmented Generation (RAG) chatbot within the published book. This chatbot, utilizing the OpenAI Agents/ChatKit SDKs, FastAPI, Neon Serverless Postgres database, and Qdrant Cloud Free Tier, must be able to answer user questions about the book's content, including answering questions based only on text selected by the user.

The Course Details:
Physical AI & Humanoid Robotics
Focus and Theme: AI Systems in the Physical World. Embodied Intelligence.
Goal: Bridging the gap between the digital brain and the physical body. Students apply their AI knowledge to control Humanoid Robots in simulated and real-world environments.
book Overview:
The future of AI extends beyond digital spaces into the physical world. This capstone book introduces Physical AI‚ÄîAI systems that function in reality and comprehend physical laws. Students learn to design, simulate, and deploy humanoid robots capable of natural human interactions using ROS 2, Gazebo, and NVIDIA Isaac.
‚Ä¢    Module 1: The Robotic Nervous System (ROS 2)
o    Focus: Middleware for robot control.
o    ROS 2 Nodes, Topics, and Services.
o    Bridging Python Agents to ROS controllers using rclpy.
o    Understanding URDF (Unified Robot Description Format) for humanoids.


‚Ä¢    Module 2: The Digital Twin (Gazebo & Unity)
o    Focus: Physics simulation and environment building.
o    Simulating physics, gravity, and collisions in Gazebo.
o    High-fidelity rendering and human-robot interaction in Unity.
o    Simulating sensors: LiDAR, Depth Cameras, and IMUs.


‚Ä¢    Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)
o    Focus: Advanced perception and training.
o    NVIDIA Isaac Sim: Photorealistic simulation and synthetic data generation.
o    Isaac ROS: Hardware-accelerated VSLAM (Visual SLAM) and navigation.
o    Nav2: Path planning for bipedal humanoid movement.


‚Ä¢    Module 4: Vision-Language-Action (VLA)
o    Focus: The convergence of LLMs and Robotics.
o    Voice-to-Action: Using OpenAI Whisper for voice commands.
o    Cognitive Planning: Using LLMs to translate natural language ("Clean the room") into a sequence of ROS 2 actions.
o    Capstone Project: The Autonomous Humanoid. A final project where a simulated robot receives a voice command, plans a path, navigates obstacles, identifies an object using computer vision, and manipulates it.
Why Physical AI Matters
Humanoid robots are poised to excel in our human-centered world because they share our physical form and can be trained with abundant data from interacting in human environments. This represents a significant transition from AI models confined to digital environments to embodied intelligence that operates in physical space.
Book Learning Outcomes for students:
1.    Understand Physical AI principles and embodied intelligence
2.    Master ROS 2 (Robot Operating System) for robotic control
3.    Simulate robots with Gazebo and Unity
4.    Develop with NVIDIA Isaac AI robot platform
5.    Design humanoid robots for natural interactions
6.    Integrate GPT models for conversational robotics
Weekly Breakdown of book content to teach students:
Weeks 1-2: Introduction to Physical AI
‚Ä¢    Foundations of Physical AI and embodied intelligence
‚Ä¢    From digital AI to robots that understand physical laws
‚Ä¢    Overview of humanoid robotics landscape
‚Ä¢    Sensor systems: LIDAR, cameras, IMUs, force/torque sensors
Weeks 3-5: ROS 2 Fundamentals
‚Ä¢    ROS 2 architecture and core concepts
‚Ä¢    Nodes, topics, services, and actions
‚Ä¢    Building ROS 2 packages with Python
‚Ä¢    Launch files and parameter management
Weeks 6-7: Robot Simulation with Gazebo
‚Ä¢    Gazebo simulation environment setup
‚Ä¢    URDF and SDF robot description formats
‚Ä¢    Physics simulation and sensor simulation
‚Ä¢    Introduction to Unity for robot visualization
Weeks 8-10: NVIDIA Isaac Platform
‚Ä¢    NVIDIA Isaac SDK and Isaac Sim
‚Ä¢    AI-powered perception and manipulation
‚Ä¢    Reinforcement learning for robot control
‚Ä¢    Sim-to-real transfer techniques
Weeks 11-12: Humanoid Robot Development
‚Ä¢    Humanoid robot kinematics and dynamics
‚Ä¢    Bipedal locomotion and balance control
‚Ä¢    Manipulation and grasping with humanoid hands
‚Ä¢    Natural human-robot interaction design

Week 13: Conversational Robotics
‚Ä¢    Integrating GPT models for conversational AI in robots
‚Ä¢    Speech recognition and natural language understanding
‚Ä¢    Multi-modal interaction: speech, gesture, vision
Assessments
‚Ä¢    ROS 2 package development project
‚Ä¢    Gazebo simulation implementation
‚Ä¢    Isaac-based perception pipeline
‚Ä¢    Capstone: Simulated humanoid robot with conversational AI

Hardware Requirements for creating Physical AI & Humanoid Robotics:
This course is technically demanding. It sits at the intersection of three heavy computational loads: Physics Simulation (Isaac Sim/Gazebo), Visual Perception (SLAM/Computer Vision), and Generative AI (LLMs/VLA).

Because the capstone involves a "Simulated Humanoid," the primary investment must be in High-Performance Workstations. However, to fulfill the "Physical AI" promise, you also need Edge Computing Kits (brains without bodies) or specific robot hardware.

1. The "Digital Twin" Workstation (Required per Student)
This is the most critical component. NVIDIA Isaac Sim is an Omniverse application that requires "RTX" (Ray Tracing) capabilities. Standard laptops (MacBooks or non-RTX Windows machines) will not work.
‚Ä¢    GPU (The Bottleneck): NVIDIA RTX 4070 Ti (12GB VRAM) or higher.
o    Why: You need high VRAM to load the USD (Universal Scene Description) assets for the robot and environment, plus run the VLA (Vision-Language-Action) models simultaneously.
o    Ideal: RTX 3090 or 4090 (24GB VRAM) allows for smoother "Sim-to-Real" training.
‚Ä¢    CPU: Intel Core i7 (13th Gen+) or AMD Ryzen 9.
o    Why: Physics calculations (Rigid Body Dynamics) in Gazebo/Isaac are CPU-intensive.
‚Ä¢    RAM: 64 GB DDR5 (32 GB is the absolute minimum, but will crash during complex scene rendering).
‚Ä¢    OS: Ubuntu 22.04 LTS.
o    Note: While Isaac Sim runs on Windows, ROS 2 (Humble/Iron) is native to Linux. Dual-booting or dedicated Linux machines are mandatory for a friction-free experience.

2. The "Physical AI" Edge Kit
Since a full humanoid robot is expensive, students learn "Physical AI" by setting up the nervous system on a desk before deploying it to a robot. This kit covers Module 3 (Isaac ROS) and Module 4 (VLA).
‚Ä¢    The Brain: NVIDIA Jetson Orin Nano (8GB) or Orin NX (16GB).
o    Role: This is the industry standard for embodied AI. Students will deploy their ROS 2 nodes here to understand resource constraints vs. their powerful workstations.
‚Ä¢    The Eyes (Vision): Intel RealSense D435i or D455.
o    Role: Provides RGB (Color) and Depth (Distance) data. Essential for the VSLAM and Perception modules.
‚Ä¢    The Inner Ear (Balance): Generic USB IMU (BNO055) (Often built into the RealSense D435i or Jetson boards, but a separate module helps teach IMU calibration).
‚Ä¢    Voice Interface: A simple USB Microphone/Speaker array (e.g., ReSpeaker) for the "Voice-to-Action" Whisper integration.
3. The Robot Lab
For the "Physical" part of the course, you have three tiers of options depending on budget.
Option A: The "Proxy" Approach (Recommended for Budget)
Use a quadruped (dog) or a robotic arm as a proxy. The software principles (ROS 2, VSLAM, Isaac Sim) transfer 90% effectively to humanoids.
‚Ä¢    Robot: Unitree Go2 Edu (~$1,800 - $3,000).
‚Ä¢    Pros: Highly durable, excellent ROS 2 support, affordable enough to have multiple units.
‚Ä¢    Cons: Not a biped (humanoid).
Option B: The "Miniature Humanoid" Approach
Small, table-top humanoids.
‚Ä¢    Robot: Unitree H1 is too expensive ($90k+), so look at Unitree G1 (~$16k) or Robotis OP3 (older, but stable, ~$12k).
‚Ä¢    Budget Alternative: Hiwonder TonyPi Pro (~$600).
o    Warning: The cheap kits (Hiwonder) usually run on Raspberry Pi, which cannot run NVIDIA Isaac ROS efficiently. You would use these only for kinematics (walking) and use the Jetson kits for AI.
Option C: The "Premium" Lab (Sim-to-Real specific)
If the goal is to actually deploy the Capstone to a real humanoid:
‚Ä¢    Robot: Unitree G1 Humanoid.
o    Why: It is one of the few commercially available humanoids that can actually walk dynamically and has an SDK open enough for students to inject their own ROS 2 controllers.

4. Summary of Architecture
To teach this successfully, your lab infrastructure should look like this:
Component    Hardware    Function
Sim Rig    PC with RTX 4080 + Ubuntu 22.04    Runs Isaac Sim, Gazebo, Unity, and trains LLM/VLA models.
Edge Brain    Jetson Orin Nano    Runs the "Inference" stack. Students deploy their code here.
Sensors    RealSense Camera + Lidar    Connected to the Jetson to feed real-world data to the AI.
Actuator    Unitree Go2 or G1 (Shared)    Receives motor commands from the Jetson.

If you do not have access to RTX-enabled workstations, we must restructure the course to rely entirely on cloud-based instances (like AWS RoboMaker or NVIDIA's cloud delivery for Omniverse), though this introduces significant latency and cost complexity.

Building a "Physical AI" lab is a significant investment. You will have to choose between building a physical On-Premise Lab at Home (High CapEx) versus running a Cloud-Native Lab (High OpEx).

Option 2 High OpEx: The "Ether" Lab (Cloud-Native)
Best for: Rapid deployment, or students with weak laptops.
1. Cloud Workstations (AWS/Azure) Instead of buying PCs, you rent instances.
‚Ä¢    Instance Type: AWS g5.2xlarge (A10G GPU, 24GB VRAM) or g6e.xlarge.
‚Ä¢    Software: NVIDIA Isaac Sim on Omniverse Cloud (requires specific AMI).
‚Ä¢    Cost Calculation:
o    Instance cost: ~$1.50/hour (spot/on-demand mix).
o    Usage: 10 hours/week √ó 12 weeks = 120 hours.
o    Storage (EBS volumes for saving environments): ~$25/quarter.
o    Total Cloud Bill: ~$205 per quarter.
2. Local "Bridge" Hardware You cannot eliminate hardware entirely for "Physical AI." You still need the edge devices to deploy the code physically.
‚Ä¢    Edge AI Kits: You still need the Jetson Kit for the physical deployment phase.
o    Cost: $700 (One-time purchase).
‚Ä¢    Robot: You still need one physical robot for the final demo.
o    Cost: $3,000 (Unitree Go2 Standard).
The Economy Jetson Student Kit
Best for: Learning ROS 2, Basic Computer Vision, and Sim-to-Real control.
Component    Model    Price (Approx.)    Notes
The Brain    NVIDIA Jetson Orin Nano Super Dev Kit (8GB)    $249    New official MSRP (Price dropped from ~$499). Capable of 40 TOPS.
The Eyes    Intel RealSense D435i    $349    Includes IMU (essential for SLAM). Do not buy the D435 (non-i).
The Ears    ReSpeaker USB Mic Array v2.0    $69    Far-field microphone for voice commands (Module 4).
Wi-Fi    (Included in Dev Kit)    $0    The new "Super" kit includes the Wi-Fi module pre-installed.
Power/Misc    SD Car
```

## Response snapshot

Created a comprehensive constitution for the Physical AI & Humanoid Robotics textbook project with six core principles: AI/Spec-Driven Development, Docusaurus Documentation First, Test-First with Interactive Elements, MCP Server Integration, Reusable Intelligence via Subagents, and User Personalization and Authentication. The constitution includes sections on Technical Requirements and Architecture, Development Workflow, and Governance with version 1.0.0 ratified on 2025-12-24.

## Outcome

- ‚úÖ Impact: Created foundational project constitution document that establishes principles for the Physical AI & Humanoid Robotics textbook development
- üß™ Tests: No tests needed for constitution creation
- üìÅ Files: Updated .specify/memory/constitution.md with the complete constitution document
- üîÅ Next prompts: Project can now proceed with spec, plan, and task creation following the established constitution
- üß† Reflection: The constitution properly captures all requirements from the user including Docusaurus usage, MCP server integration, authentication with Better-Auth.com, personalization features, and RAG chatbot development

## Evaluation notes (flywheel)

- Failure modes observed: None
- Graders run and results (PASS/FAIL): N/A
- Prompt variant (if applicable): N/A
- Next experiment (smallest change to try): Begin creating feature specifications based on the established constitution